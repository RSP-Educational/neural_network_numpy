{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9c163b",
   "metadata": {},
   "source": [
    "<h1>Coding Session #2 - Neuronale Netze</h1>\n",
    "\n",
    "Diese Datei ist ein Jupyter Notebook. Dieses besteht aus Textblöcken im Markdown Format und ausführbaren Code-Zellen. Diese erkennen Sie an dem kleinen Pfeilsymbol links daneben.\n",
    "\n",
    "Führen Sie bitte als erstes folgende Zelle aus, um sicherzustellen, dass die benötigten Bibliotheken installiert sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06aadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dca9c5",
   "metadata": {},
   "source": [
    "## Ziel\n",
    "In dieser Coding Session soll ein neuronales Netz von Grund auf implementiert und trainiert werden. Sie wenden dabei die in der Vorlesung gelernten theoretischen Grundlagen praktisch an und lernen den Einfluss verschiedener Faktoren auf das Trainingsverhalten kennen.\n",
    "\n",
    "## 1 Neuronales Netz\n",
    "\n",
    "Ein neuronales Netz besteht aus mehreren Schichten. Jede Schicht (Layer) beinhaltet mindestens ein Neuron.\n",
    "\n",
    "<img src=\"images/neural_network1.png\" width=\"300px\"></img>\n",
    "\n",
    "Jeder Layer sagt für einen Input $X$ einen Output $\\hat{Y}$ vorher: $\\hat{Y}=W\\cdot X+b$\n",
    "\n",
    "In einem neuronalen Netz verarbeitet jeder Layer den Output des davorliegenden Layers (bzw. der erste Layer den Netzwerk Input $X$), wobei $H$ sogenannte Hidden Layer sind.\n",
    "\n",
    "<img src=\"images/neural_network.png\" width=\"300px\"></img>\n",
    "\n",
    "Damit ergibt sich die gesamte Netzwerkfunktion als\n",
    "\n",
    "$H=W_h\\cdot X+b_h$\n",
    "\n",
    "$\\hat{Y}=W_{out}\\cdot H + b_{out}$\n",
    "\n",
    "Die Backpropagation erfolgt Layerweise.\n",
    "\n",
    "## 2 Code\n",
    "### 2.1 Basisklasse Module\n",
    "\n",
    "Wir starten mit der Klasse `Module`. Dies ist die Basisklasse, von der alle weiteren Netzwerkkomponenten erben. Sie beinhaltet ein Grundgerüst, welches in allen Aktivierungs- und Verlustfunktionen sowie in allen Netzwerklayern vorhanden sein muss. Dieses besteht aus\n",
    "\n",
    "`forward`: die Funktion, die während der Forward Propagation im Netzwerk aufgerufen wird, um Vorhersagen zu treffen<br>\n",
    "`backward`: die Funktion, die während der Backward Propagation aufgerufen wird, um die Parameteranpassungen vorzunehmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbff489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Base Module Class -> all layers and activations will inherit from this\n",
    "class Module:\n",
    "    \"\"\"Base class for all modules (layers, activations, etc.)\"\"\"\n",
    "    def forward(self, input:np.ndarray):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad_output, eta = None):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce65625",
   "metadata": {},
   "source": [
    "### 2.2 Netzwerschichten\n",
    "\n",
    "Kommen wir nun zu den Netzwerkschichten. Wir beschränken uns hier erst einmal auf den `DenseLayer` (oder Fully Connected Layer), also den einfachsten Schichttyp.\n",
    "\n",
    "Dieser bekommt im Konstruktor die Parameter `input_size` und `output_size` übergeben, womit die Gewichtsmatrix $W$ und der Biasvektor $b$ initialisiert werden.\n",
    "\n",
    "**Beispiel:** Folgender Layer hat 3 Inputs und 2 Ouputs.\n",
    "\n",
    "<img src=\"images/neural_network1.png\" width=\"300px\"></img>\n",
    "\n",
    "Unsere Gewichtsmatrix (`self.weights`) wird mit normalverteilten Zufallswerten dem shape $(input\\_size, output\\_size)$ initialisiert, sieht folglich so aus:\n",
    "$$\n",
    "    W=\\begin{bmatrix}\n",
    "        W_{00} & W_{10} \\\\\n",
    "        W_{01} & W_{11} \\\\\n",
    "        W_{02} & W_{12}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Unser Biasvektor (`self.biases`) wird mit Nullen mit dem shape $(1, output\\_size)$ initialisiert und sieht so aus:\n",
    "$$\n",
    "    b=\\begin{bmatrix}\n",
    "        b_0 & b_1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 2.2.1 Die `forward` Funktion (Forward Propagation)\n",
    "\n",
    "Die übergebenen Inputs $X$ (`inputs`) liegen batchweise vor (um mehrere Inputs gleichzeitig verarbeiten zu können). Folglich hat der Inputvektor einen shape von $(n, input\\_size)$ und sieht so aus:\n",
    "$$\n",
    "    X=\\begin{bmatrix}\n",
    "        x_0 & x_1 & x_2 \\\\\n",
    "        x_0 & x_1 & x_2 \\\\\n",
    "        ...   &     &     \\\\\n",
    "        x_0 & x_1 & x_2\n",
    "    \\end{bmatrix}\n",
    "    \\begin{matrix}\n",
    "        batch\\,item\\,0 \\\\\n",
    "        batch\\,item\\,1 \\\\\n",
    "        ... \\\\\n",
    "        batch\\,item\\,n\n",
    "    \\end{matrix}\n",
    "$$\n",
    "wobei $n$ die Anzahl der Input Items im Batch ist.\n",
    "\n",
    "Die shapes können also nicht nach der Form $\\hat{Y}=W\\cdot X+b$ verarbeitet werden. Wir wenden hier einen kleinen Trick an und rechnen $\\hat{Y}=X\\cdot W +b$. So können gestackte Inputs (batches) parallel verarbeitet werden, statt mit einer Schleife über jedes $batch\\,item$ zu iterieren. Das Prinzip bleibt aber das gleiche und die Ergebnisse sind identisch und die Verarbeitung ist um ein Vielfaches performanter!\n",
    "\n",
    "Die Outputs $\\hat{Y}$ liegen dann in folgender Form vor:\n",
    "$$\n",
    "    \\hat{Y}=\\begin{bmatrix}\n",
    "        y_0 & y_1 \\\\\n",
    "        y_0 & y_1 \\\\\n",
    "        ... \\\\\n",
    "        y_0 & y_1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{matrix}\n",
    "        batch\\,item\\,0 \\\\\n",
    "        batch\\,item\\,1 \\\\\n",
    "        ... \\\\\n",
    "        batch\\,item\\,n\n",
    "    \\end{matrix}\n",
    "$$\n",
    "\n",
    "In der `forward` Funktion müssen die Inputs mit `self.inputs = inputs.copy()` gespeichert werden. Diese brauchen wir später in der `backward` Funktion, um die Gradienten zu berechnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276a46e",
   "metadata": {},
   "source": [
    "> <span style=\"color:#00A1E3\">**Aufgabe 1 - Forward Propagation**</span>\n",
    ">\n",
    "> <img src=\"images/task_1_1.png\" height=\"260px\"></img>\n",
    ">\n",
    "> Führen Sie folgende Zelle aus, um die Lösung anzuzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities.test as test\n",
    "\n",
    "test.task_1_1_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e000d5c",
   "metadata": {},
   "source": [
    "#### 2.2.2 Die `backward` Funktion (Backpropagation)\n",
    "\n",
    "Die Gradienten werden Schichtweise zurückpropagiert, um die Parameter der einzelnen Layer anzupassen. Die `backward` Funktion bekommt den Gradienten der nachfolgenden Schicht $\\frac{\\partial L}{\\partial Y}$ (`grad_output`) und die Lernrate $\\eta$ (`eta`) übergeben. Der Gradient hat einen shape von $(batch\\_size, output\\_size)$ und sieht so aus:\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial Y}=\\begin{bmatrix}\n",
    "        g_0 & g_1 \\\\\n",
    "        g_0 & g_1 \\\\\n",
    "        \\dots \\\\\n",
    "        g_o & g_1\n",
    "    \\end{bmatrix}\n",
    "    \\begin{matrix}\n",
    "        batch\\,item\\,0 \\\\\n",
    "        batch\\,item\\,1 \\\\\n",
    "        ... \\\\\n",
    "        batch\\,item\\,n\n",
    "    \\end{matrix}\n",
    "$$\n",
    "\n",
    "Als erstes berechnen wir den Gewichtsgradienten (`grad_weights`), also die partielle Ableitung nach $W$\n",
    "$$\n",
    "    \\Delta W\\approx\\frac{\\partial L}{\\partial W}=X^T\\cdot\\frac{\\partial L}{\\partial Y}\n",
    "$$\n",
    "> Hinweis: Durch die Anwendung des Skalarproduktes wird die Summe über die Batch Dimension gebildet. Somit hat $\\Delta W$ automatisch den gleichen shape wie $W$, da die Batch Dimension entfällt. Somit ist das Parameterupdate für $W$ später problemlos möglich.\n",
    "\n",
    "Nun werden die Biasgradienten (`grad_biases`) berechnet, also die partielle Ableitung nach $b$\n",
    "$$\n",
    "    \\Delta b\\approx\\frac{\\partial L}{\\partial b}=\\sum_{batch}\\frac{\\partial L}{\\partial Y}\n",
    "$$\n",
    "Für den Biasgradienten bilden wir also einfach die Summe über alle Batch Items des Output Gradienten (`grad_output`), um hier ebenfalls eine parallele Verarbeitung aller Batch Items zu gewährleisten und die Batch Dimension zu eliminieren.\n",
    "\n",
    "Anschließend wird noch der Inputgradient (`grad_input`) berechnet. Dies ist der Rückgabewert der Funktion, welcher dann an den vorgelagerten Layer weitergegeben wird:\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial X}=\\frac{\\partial L}{\\partial Y}\\cdot W^T\n",
    "$$\n",
    "\n",
    "Abschließend erfolgt natürlich noch das Parameter Update:\n",
    "$$\n",
    "    W=W-\\eta\\cdot\\Delta W\n",
    "$$\n",
    "$$\n",
    "    b=b-\\eta\\cdot\\Delta b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7906a",
   "metadata": {},
   "source": [
    "> <span style=\"color:#00A1E3\">**Aufgabe 2 - Mean Squared Error**</span>\n",
    "> \n",
    "> <img src=\"images/task_1_2.png\" height=\"260\"></img>\n",
    ">\n",
    "> Führen Sie folgende Zelle aus, um die Lösung anzuzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59831bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities.test as test\n",
    "\n",
    "test.task_1_2_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6743e9",
   "metadata": {},
   "source": [
    "> <span style=\"color:#00A1E3\">**Aufgabe 3 - Gradienten-Berechnung**</span>\n",
    ">\n",
    "> <img src=\"images/task_1_3.png\" height=\"160\"></img>\n",
    ">\n",
    "> Führen Sie folgende Zelle aus, um die Lösung anzuzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities.test as test\n",
    "\n",
    "test.task_1_3_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fc2bc2",
   "metadata": {},
   "source": [
    "> <span style=\"color:#00A1E3\">**Aufgabe 4 - Dense Layer**</span>\n",
    ">\n",
    "> 1. Erstellen Sie eine Klasse `DenseLayer`, welche von `Module` ableitet.\n",
    "> 2. Erstellen Sie den Konstruktor (`def __init__(self, input_size, out_put_size)`)\n",
    ">    - Initialisieren Sie die Gewichtsmatrix $W$ (`self.weights`) mit Hilfe der Funktion `np.random.randn`. Übergeben Sie für den shape `input_size` und `output_size`.\n",
    ">    - Multiplizieren Sie die Gewichtsmatrix mit `0.1`, um nicht mit zu großen Initialgewichten zu starten $\\to$ stabileres Training\n",
    ">    - Initialisieren Sie die Biases $b$ (`self.biases`) mit Null (`np.zeros`) mit einem shape von $(1,\\,input\\_size)$\n",
    "> 3. Erstellen Sie die Funktion `forward(self, input)` für die Vorwärtspropagation\n",
    ">    - Speichern Sie eine Kopie der `inputs` in `self.inputs`, um diese später für die Backpropagation zur Verfügung zu haben (nutzen Sie `np.copy()`)\n",
    ">    - Geben Sie den Output in der Form $\\hat{Y}=X\\cdot W+b$ zurück (nutzen Sie für das Skalarprodukt `np.dot()`)\n",
    "> 4. Erstellen Sie die Funktion `backward(self, grad_output, eta)` für die Backpropagation. `grad_output` ist hier der Gradient der Nachfolgeschicht $\\frac{\\partial L}{\\partial Y}$.\n",
    ">    - Berechnen Sie die Gewichtsgradienten $\\frac{\\partial L}{\\partial W}$ (`grad_weights`) nach der Formel $\\Delta W\\approx\\frac{\\partial{L}}{\\partial W}=X^T\\cdot\\frac{\\partial L}{\\partial Y}$\n",
    ">    - Berechnen Sie die Biasgradienten $\\frac{\\partial L}{\\partial b}$ (`grad_biases`) nach der Formel $\\Delta b\\approx\\frac{\\partial L}{\\partial b}=\\sum_{batch}\\frac{\\partial L}{\\partial Y}$. Nutzen Sie für die Summe `np.sum(..., axis=0, keepdims=True)`\n",
    ">    - Berechnen Sie den Input Gradienten (`grad_input`) $\\frac{\\partial L}{\\partial X}=\\frac{\\partial L}{\\partial Y}\\cdot W^T$. Dies ist der Rückgabewert der Funktion, welcher anschließend vom vorgelagerten Layer weiterverarbeitet wird.\n",
    ">    - Passen Sie nun die Gewichte (`self.weights`) und Biase (`self.biases`) an:\n",
    ">      - $W=W-\\eta\\cdot\\Delta W$\n",
    ">      - $b=b-\\eta\\cdot \\Delta b$\n",
    ">    - Geben Sie den Inputgradienten am Ende der Funktion zurück"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26381d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b098b3a3",
   "metadata": {},
   "source": [
    "Glückwunsch, Sie haben nun ihren ersten Fully Connected Layer implementiert - die Basis für den weiteren Lernprozess! Und damit ist auch schon der komplizierteste Teil erledigt.\n",
    "<br><br><br>\n",
    "**Test der Implementierung**\n",
    "\n",
    "Mit folgender Zelle können Sie Ihren Code testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f97024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.data import generate_linear_data\n",
    "from utilities.visualization import plot_data_points\n",
    "\n",
    "EPOCHS          = 40\n",
    "LEARNING_RATE   = 0.1\n",
    "N               = 20\n",
    "\n",
    "layer = DenseLayer(1, 1)\n",
    "\n",
    "X, Y = generate_linear_data(m=0.5, n=-1.0, num_samples=N, noise=0.02)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # Forward pass\n",
    "    Y_hat = layer.forward(X)\n",
    "    \n",
    "    # Compute Mean Squared Error Loss\n",
    "    loss = np.mean((Y_hat - Y) ** 2)\n",
    "    \n",
    "    # Backward pass\n",
    "    grad_loss = 2 * (Y_hat - Y) / N\n",
    "    layer.backward(grad_loss, LEARNING_RATE)\n",
    "\n",
    "    # Visualization\n",
    "    if epoch%10 == 0 or epoch == 1 or epoch == EPOCHS:\n",
    "        plot_data_points(X, Y, Y_hat, title=f\"Epoch {epoch}: Model Predictions\")    \n",
    "    print(f\"Epoch {epoch}/{EPOCHS}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3179331a",
   "metadata": {},
   "source": [
    "Der Loss sollte nach 40 Epochen $\\le 0.0015$ sein und die Vorhersage (Prediction) sollte die Daten (Ground Truth) relativ gut abbilden.\n",
    "\n",
    "### 2.3 Aktivierungsfunktionen\n",
    "\n",
    "Aktivierungsfunktionen sind ein wichtiger Bestandteil des maschinellen Lernens. Ohne sie wären Neuronen darauf beschränkt, lineare Beziehungen zwischen Inputs und Outputs zu lernen. Durch die Einführung von Nichtlinearitäten werden neuronale Netze in die Lage versetzt, hochkomplexe Zusammenhänge zu lernen.\n",
    "\n",
    "<img src=\"images/activation_function.png\" width=\"500px\"></img>\n",
    "\n",
    "Aktivierungsfunktionen werden den Neuronen nachgeschaltet und transformieren deren linearen Output $\\hat{y}$ in Nichtlinearitäten $g(\\hat{y})$.\n",
    "\n",
    "#### 2.3.1 Rectified Linear Unit (ReLU)\n",
    "\n",
    "_ReLU_ ist die verbreiteteste Aktivierungsfunktion. Eingesetzt wird _ReLU_ vor allem in versteckten Schichten (Hidden Layers) tiefer neuronaler Netze.<br>\n",
    "<img src=\"images/ReLU.png\" width=\"500px\"></img>\n",
    "\n",
    "_ReLU_ setzt alle Inputs kleiner $0$ auf $0$ und verhält sich im positiven Bereich linear mit einer Steigung von $1$. Der Gradient von __ReLU__ ist $0$ für alle $z<0$, anderenfalls $1$.\n",
    "\n",
    "> <span style=\"color:#00A1E3\">**Aufgabe 5 - ReLU**</span>\n",
    "> 1. Implementieren Sie die Klasse `ReLU`, welche von `Module` ableitet\n",
    "> 2. Implementieren Sie die Funktion `forward(self, inputs)`\n",
    ">    - Speichern Sie die `inputs` in `self.inputs`, um diese später für die Backpropagation zur Verfügung zu haben (nutzen Sie `np.copy()`)\n",
    ">    - Setzen Sie alle Input Werte, welche kleiner $0$ sind, auf $0$ und geben Sie die `inputs` am Ende der Funktion zurück\n",
    "> 3. Implementieren Sie die Funktion `backward(self, grad_output)`\n",
    ">    - Kopieren Sie den Output Gradienten in `grad_input`\n",
    ">    - Setzen Sie für `grad_input` alle Elemente, die `self.inputs <= 0` entsprechen auf $0$\n",
    ">    - Geben Sie den Input Gradienten am Ende der Funktion zurück"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af44bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b42b3",
   "metadata": {},
   "source": [
    "**Test der Implementierung**\n",
    "\n",
    "Mit folgender Zelle können Sie Ihren Code testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities.test as test\n",
    "\n",
    "relu = ReLU()\n",
    "test.test_ReLU(relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b860e98",
   "metadata": {},
   "source": [
    "#### 2.3.2 Sigmoid\n",
    "\n",
    "_Sigmoid_ hat einen S-förmigen Verlauf. Die Outputs sind positive Werte $0<g(z)<1$. _Sigmoid_ wird vor allem genutzt, um numerische Netzwerk Outputs $[-\\infty; +\\infty]$ in Wahrscheinlichkeitswerte $[0; 1]$ umzuwandeln.\n",
    "\n",
    "<img src=\"images/sigmoid.png\" width=\"550px\"></img>\n",
    "\n",
    "\n",
    "> <span style=\"color:#00A1E3\">**Aufgabe 6 - Sigmoid**</span>\n",
    "> \n",
    "> 1. Implementieren Sie die Klasse `Sigmoid`, welche von `Module` ableitet\n",
    "> 2. Implementieren Sie die Funktion `forward(self, inputs)`\n",
    ">    - Berechnen Sie den Output nach der Formel $g(z)=\\frac{1}{1+exp(-z)}$\n",
    ">    - Speichern Sie die Outputs ($g(z)$) in `self.outputs`, um diese später für die Backpropagation zur Verfügung zu haben\n",
    "> 3. Implementieren Sie die Funktion `backward(self, grad_output)`\n",
    ">    - Berechnen Sie den Input Gradienten nach $g'(z)=g(z)\\cdot(1-g(z))$ und geben Sie diesen zurück"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f912a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ebe5b2c",
   "metadata": {},
   "source": [
    "**Test der Implementierung**\n",
    "\n",
    "Mit folgender Zelle können Sie Ihren Code testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4441512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities.test as test\n",
    "\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "test.test_sigmoid(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b371b7",
   "metadata": {},
   "source": [
    "#### 2.3.3 Tanh\n",
    "\n",
    "_Tanh_ hat wie _Sigmoid_ einen S-förmigen Verlauf. Die Outputs sind Werte $-1<g(z)<1$. _Tanh_ kommt u. a. in Hidden Layers flacherer Netze zum Einstz.\n",
    "\n",
    "<img src=\"images/tanh.png\" width=\"550px\"></img>\n",
    "\n",
    "> <span style=\"color:#00A1E3\">**Aufgabe 7 - Tanh**</span>\n",
    "> \n",
    "> 1. Implementieren Sie die Klasse `Tanh`, welche von `Module` ableitet\n",
    "> 2. Implementieren Sie die Funktion `forward(self, inputs)`\n",
    ">    - Berechnen Sie den Output nach der Formel $g(z)=tanh(z)$\n",
    ">    - Speichern Sie die Outputs ($g(z)$) in `self.outputs`, um diese später für die Backpropagation zur Verfügung zu haben\n",
    "> 3. Implementieren Sie die Funktion `backward(self, grad_output)`\n",
    ">    - Berechnen Sie den Input Gradienten nach $g'(z)=1-tanh^2(z)$ und geben Sie diesen zurück"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087965e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "302ea52c",
   "metadata": {},
   "source": [
    "**Test der Implementierung**\n",
    "\n",
    "Mit folgender Zelle können Sie Ihren Code testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ec362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities.test as test\n",
    "\n",
    "tanh = Tanh()\n",
    "\n",
    "test.test_tanh(tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538fb41",
   "metadata": {},
   "source": [
    "### 2.4 Verlustfunktionen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e52b2a",
   "metadata": {},
   "source": [
    "> <span style=\"color:#00A1E3\">**Aufgabe 8 - MSE Loss**</span>\n",
    ">\n",
    "> 1. Implementieren Sie `MSELoss`, welcher von `Module` ableitet.\n",
    "> 2. Implementieren Sie die Funktion `forward(self, prediction, target)`\n",
    ">    - Berechnen Sie den MSE Loss nach der Formel $L_{MSE}=\\frac{1}{n}\\sum_{i=1}^{n}(Y_i-\\hat{Y}_i)^2$ und geben Sie diesen zurück\n",
    "> 3. Implementieren Sie die Funktion `backward(self, prediction, target)`\n",
    ">    - Berechnen Sie die Input Gradienten nach $\\frac{\\partial L}{\\partial Y}=2\\cdot(\\hat{Y}-{Y})$ und geben Sie diese zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa75b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f578fa",
   "metadata": {},
   "source": [
    "### 2.5 Neuronales Netz\n",
    "\n",
    "Im neuronalen Netz befinden sich mehrere Neuronenschichten. Dieses bietet eine Wrapper-Funktionalität, um die Forward- sowie die Backpropagation für alle Layer auszuführen. Dafür wird der Input an die `forward`Funktion des ersten Layers übergeben, dessen Output als Input des darauffolgenden Layers fungiert, usw..\n",
    "\n",
    "In der Backpropagation wird der Loss-Gradient Layerweise zurückpropagiert. Dafür wird er zuerst an die `backward` Funktion des letzten Layers übergeben. Diese gibt wiederum einen Gradienten zurück, welche an die `backward` Funktion der davorliegenden Schicht weitergegeben wird, usw..\n",
    "\n",
    "Zwischen den Layern können sich Aktivierungsfunktionen befinden, die nach dem gleichen Prinzip funktionieren.\n",
    "\n",
    "> <span style=\"color:#00A1E3\">**Aufgabe 9 - Neuronales Netz**</span>\n",
    ">\n",
    "> 1. Implementieren Sie `NeuralNetwork`, welches von `Module` ableitet.\n",
    "> 2. Erstellen Sie den Konstruktor (`def __init__(self, modules:list[Module])`)\n",
    ">    - `modules` ist hierbei eine Liste von Objekten der Klasse `Module`. Dies können sowohl Layer als auch Aktivierungsfunktionen sein.\n",
    ">    - Speichern Sie diese in `self.modules`, um später in der `forward` und `backward` Funktion darauf zugreifen zu können\n",
    "> 3. Erstellen Sie die Funktion `forward(self, input)` für die Vorwärtspropagation\n",
    ">    - Iterieren Sie über `self.modules` und rufen Sie für jedes Modul die `forward` Methode auf. Übergeben Sie dieser den `input` als Parameter.\n",
    ">    - Speichern Sie den Rückgabewert der `forward` Funktion wiederum in Input (Sie überschreiben hier den alten Wert)\n",
    ">    - Geben Sie am Schluss der Funktion `input` zurück. Dies ist unsere finale Modellvorhersage $\\hat{Y}$\n",
    "> 4. Erstellen Sie die Funktion `backward(self, grad_output, eta)` für die Backpropagation. `grad_output` ist hier der Gradient der Lossfunktion $\\frac{\\partial L}{\\partial Y}$.\n",
    ">    - Da die Gradienten von der letzten Schicht zur ersten zurückpropagiert werden, muss zuerst das letzte Modul, dann das vorletzte, usw. aufgerufen werden.\n",
    ">    - Iterieren Sie hierfür bitte in invertierter Reihenfolge über `self.modules`, indem Sie Python Funktion `reversed(self.modules)` nutzen\n",
    ">    - Da sich in `self.modules` sowohl Aktivierungsfunktionen als auch Layer befinden, muss innerhalb der Iterationsschleife folgende Unterscheidung getroffen werden:\n",
    ">       - Ist das Modul vom Typ `DenseLayer`, so erwartet dessen `backward` Funktion die Parameter `grad_output` und `eta`\n",
    ">       - Für Aktivierungsfunktionen wird nur der Parameter `grad_output` erwartet\n",
    ">       - Prüfen Sie deshalb mit `if isinstance(module, DenseLayer)`, ob es sich um einen Layer handelt und rufen Sie die `module.backward` Funktion mit beiden Paremetern auf, anderenfalls nur mit `grad_output`\n",
    ">    - Speichern Sie den Rückgabewert der `backward` Funktion in `grad_output`. Sie überschreiben hier den alten Wert, um den Gradienten des entsprechenden Moduls an das vorhergehende Modul zu übergebn\n",
    ">    - Geben Sie am Ende der Funktion `grad_output` zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6585ca",
   "metadata": {},
   "source": [
    "### 2.6 Training\n",
    "\n",
    "Nun testen wir die Implementierung. Dafür werden sinusförmige Trainingsdaten mit Noise generiert. Das Modell soll lernen, diese Daten vorherzusagen.\n",
    "\n",
    "> <span style=\"color:#00A1E3\">**Aufgabe 10 - Training**</span>\n",
    "> 1. Erstellen Sie eine Instanz der Klasse `NeuralNetwork`\n",
    ">   - Für `layers` übergeben Sie eine Liste, 2-4 Neuronenschichten (`DenseLayer`) beinhaltet\n",
    ">   - Hierbei soll auf jeden `DenseLayer` eine Aktivierungsfunktion folgen (also `ReLU`, `Sigmoid`oder `Tanh`)\n",
    ">   - Verzichten Sie nach dem letzten Layer auf die Aktivierungsfunktion\n",
    ">   - Übergeben Sie jedem `DenseLayer` die Anzahl der Inputneuronen sowie die Anzahl der Outputneuronen.\n",
    ">       - Die erste Neuronenschicht hat lediglich 1 Inputneuron\n",
    ">       - Die letzte Neuronenschicht hat lediglich 1 Outputneuron\n",
    ">       - Jede Anzahl der Inputneuronen jeder Folgeschicht muss der Anzahl der Outputneuronen der vorherigen entsprechen\n",
    "> 2. Experimentieren Sie mit der Modellarchitektur.\n",
    "> 3. Finden Sie geeignete Werte für die Anzahl der Trainingsepochen (`EPOCHS`), die Lernrate $\\eta$ (`LEARNING_RATE`) und die Batchgröße (`BATCH_SIZE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b72f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.data import generate_sinusoidal_data                 # Custom module for generating sinusoidal data\n",
    "from utilities.visualization import plot_data_points, plot_series   # Custom module for visualizing 2D data\n",
    "\n",
    "N               :int    = 1000      # Number of data points  \n",
    "EPOCHS          :int    = None      # Number of training epochs\n",
    "LEARNING_RATE   :float  = None      # Learning Rate\n",
    "BATCH_SIZE      :int    = None      # Batch size for training\n",
    "\n",
    "# Generate sinusoidal data\n",
    "X, Y = generate_sinusoidal_data(\n",
    "    num_samples = N,\n",
    "    noise       = 0.05\n",
    ")\n",
    "\n",
    "# plot data points\n",
    "plot_data_points(X, Y)\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    layers = [\n",
    "        # Your code here\n",
    "    ]\n",
    ")\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    loss_epoch = 0.\n",
    "    indices = np.random.choice(range(N), N, replace=False)\n",
    "    Y_hat = np.zeros_like(Y)\n",
    "\n",
    "    for batch in range(0, N, BATCH_SIZE):\n",
    "        batch_indices = indices[batch:batch + BATCH_SIZE]\n",
    "        X_batch = X[batch_indices]\n",
    "        Y_batch = Y[batch_indices]\n",
    "\n",
    "        # Forward pass\n",
    "        Y_hat_batch = model.forward(X_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn.forward(Y_hat_batch, Y_batch)\n",
    "        loss_epoch += loss\n",
    "\n",
    "        # Backward pass\n",
    "        grad_loss = loss_fn.backward(Y_hat_batch, Y_batch)\n",
    "        model.backward(grad_loss, LEARNING_RATE)\n",
    "\n",
    "        # store predictions\n",
    "        Y_hat[batch_indices] = Y_hat_batch\n",
    "    loss_epoch /= (N / BATCH_SIZE)\n",
    "    losses.append(loss_epoch)\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1 or epoch == EPOCHS:\n",
    "        plot_data_points(input=X, target=Y, prediction=Y_hat, title=f\"Sinusoidal Data Fitting - Epoch {epoch}\")\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}, MSE: {loss_epoch:0.6f}\")\n",
    "\n",
    "plot_series(data=losses, title=\"Training Loss over Epochs\", xlabel=\"Epochs\", ylabel=\"MSE Loss\")\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97e7786",
   "metadata": {},
   "source": [
    "> <span style=\"color:#00A1E3\">**Aufgabe 11 - Experimente**</span>\n",
    "> 1. Entfernen Sie die Aktivierungsfunktionen zwischen den Layern. Was passiert?\n",
    "> 2. Ersetzen Sie die `ReLU`Aktivierungen durch `Tanh`und anschließen durch `Sigmoid`. Welchen Effekt hat dies auf das Ergebnis (`MSE` Werte vergleichen)?\n",
    "> 3. Setzen Sie die `BATCHSIZE=N`, um während des Trainings nicht batchweise, sondern auf dem gesamten Datensatz zu optimieren. Was passiert?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
